import torch as t
import torch.nn as nn
import torch.nn.functional as F


def test_trace(trace_fn):
    for n in range(10):
        assert (
            trace_fn(t.zeros((n, n), dtype=t.long)) == 0
        ), f"Test failed on zero matrix with size ({n}, {n})"
        assert (
            trace_fn(t.eye(n, dtype=t.long)) == n
        ), f"Test failed on identity matrix with size ({n}, {n})"
        x = t.randint(0, 10, (n, n))
        expected = t.trace(x)
        actual = trace_fn(x)
        assert (
            actual == expected
        ), f"Test failed on randmly initialised matrix with size ({n}, {n})"
    print("All tests in `test_trace` passed!")


def test_mv(mv_fn):
    mat = t.randn(3, 4)
    vec = t.randn(4)
    mv_actual = mv_fn(mat, vec)
    mv_expected = mat @ vec
    t.testing.assert_close(mv_actual, mv_expected)
    print("All tests in `test_mv` passed!")


def test_mv2(mv_fn):
    big = t.randn(30)
    mat = big.as_strided(size=(3, 4), stride=(2, 4), storage_offset=8)
    vec = big.as_strided(size=(4,), stride=(3,), storage_offset=8)
    mv_actual = mv_fn(mat, vec)
    mv_expected = mat @ vec
    t.testing.assert_close(mv_actual, mv_expected)
    print("All tests in `test_mv2` passed!")


def test_mm(mm_fn):
    matA = t.randn(3, 4)
    matB = t.randn(4, 5)
    mm_actual = mm_fn(matA, matB)
    mm_expected = matA @ matB
    t.testing.assert_close(mm_actual, mm_expected)
    print("All tests in `test_mm` passed!")


def test_mm2(mm_fn):
    big = t.randn(30)
    matA = big.as_strided(size=(3, 4), stride=(2, 4), storage_offset=8)
    matB = big.as_strided(size=(4, 5), stride=(3, 2), storage_offset=8)
    mm_actual = mm_fn(matA, matB)
    mm_expected = matA @ matB
    t.testing.assert_close(mm_actual, mm_expected)
    print("All tests in `test_mm2` passed!")


def test_conv1d_minimal_simple(conv1d_minimal_simple, n_tests=5):
    import numpy as np

    for _ in range(n_tests):
        h = np.random.randint(10, 30)
        kernel_size = np.random.randint(1, 10)
        x = t.randn((h,))
        weights = t.randn((kernel_size,))
        my_output = conv1d_minimal_simple(x, weights)
        torch_output = t.conv1d(
            x.unsqueeze(0).unsqueeze(0),
            weights.unsqueeze(0).unsqueeze(0),
            stride=1,
            padding=0,
        ).squeeze()
        t.testing.assert_close(my_output, torch_output)
    print("All tests in `test_conv1d_minimal_simple` passed!")


def test_conv1d_minimal(conv1d_minimal, n_tests=20):
    import numpy as np

    for _ in range(n_tests):
        b = np.random.randint(1, 10)
        h = np.random.randint(10, 30)
        ci = np.random.randint(1, 5)
        co = np.random.randint(1, 5)
        kernel_size = np.random.randint(1, 10)
        x = t.randn((b, ci, h))
        weights = t.randn((co, ci, kernel_size))
        my_output = conv1d_minimal(x, weights)
        torch_output = t.conv1d(x, weights, stride=1, padding=0)
        t.testing.assert_close(my_output, torch_output)
    print("All tests in `test_conv1d_minimal` passed!")


def test_conv2d_minimal(conv2d_minimal, n_tests=4):
    """
    Compare against torch.conv2d.
    Due to floating point rounding, they can be quite different in float32 but should be nearly identical in float64.
    """
    import numpy as np

    for i in range(n_tests):
        b = np.random.randint(1, 10)
        h = np.random.randint(10, 300)
        w = np.random.randint(10, 300)
        ci = np.random.randint(1, 20)
        co = np.random.randint(1, 20)
        kernel_size = tuple(np.random.randint(1, 10, size=(2,)))
        x = t.randn((b, ci, h, w), dtype=t.float64)
        weights = t.randn((co, ci, *kernel_size), dtype=t.float64)
        my_output = conv2d_minimal(x, weights)
        torch_output = t.conv2d(x, weights)
        t.testing.assert_close(my_output, torch_output)
    print("All tests in `test_conv2d_minimal` passed!")


def test_conv1d(conv1d, n_tests=10):
    import numpy as np

    for i in range(n_tests):
        b = np.random.randint(1, 10)
        h = np.random.randint(10, 300)
        ci = np.random.randint(1, 20)
        co = np.random.randint(1, 20)
        stride = np.random.randint(1, 5)
        padding = np.random.randint(0, 5)
        kernel_size = np.random.randint(1, 10)
        x = t.randn((b, ci, h))
        weights = t.randn((co, ci, kernel_size))
        my_output = conv1d(x, weights, stride=stride, padding=padding)
        torch_output = t.conv1d(x, weights, stride=stride, padding=padding)
        t.testing.assert_close(my_output, torch_output, atol=1e-4, rtol=1e-4)
    print("All tests in `test_conv1d` passed!")


def test_pad1d(pad1d):
    """Should work with one channel of width 4."""
    x = t.arange(4).float().view((1, 1, 4))
    actual = pad1d(x, 1, 3, -2.0)
    expected = t.tensor([[[-2.0, 0.0, 1.0, 2.0, 3.0, -2.0, -2.0, -2.0]]])
    t.testing.assert_close(actual, expected)
    actual = pad1d(x, 1, 0, -2.0)
    expected = t.tensor([[[-2.0, 0.0, 1.0, 2.0, 3.0]]])
    t.testing.assert_close(actual, expected)
    print("All tests in `test_pad1d` passed!")


def test_pad1d_multi_channel(pad1d):
    """Should work with two channels of width 2."""
    x = t.arange(4).float().view((1, 2, 2))
    actual = pad1d(x, 0, 2, -3.0)
    expected = t.tensor([[[0.0, 1.0, -3.0, -3.0], [2.0, 3.0, -3.0, -3.0]]])
    t.testing.assert_close(actual, expected)
    print("All tests in `test_pad1d_multi_channel` passed!")


def test_pad2d(pad2d):
    """Should work with one channel of 2x2."""
    x = t.arange(4).float().view((1, 1, 2, 2))
    expected = t.tensor(
        [
            [
                [
                    [0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0],
                    [0.0, 1.0, 0.0],
                    [2.0, 3.0, 0.0],
                    [0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0],
                    [0.0, 0.0, 0.0],
                ]
            ]
        ]
    )
    actual = pad2d(x, 0, 1, 2, 3, 0.0)
    t.testing.assert_close(actual, expected)
    print("All tests in `test_pad2d` passed!")


def test_pad2d_multi_channel(pad2d):
    """Should work with two channels of 2x1."""
    x = t.arange(4).float().view((1, 2, 2, 1))
    expected = t.tensor(
        [
            [
                [[-1.0, 0.0], [-1.0, 1.0], [-1.0, -1.0]],
                [[-1.0, 2.0], [-1.0, 3.0], [-1.0, -1.0]],
            ]
        ]
    )
    actual = pad2d(x, 1, 0, 0, 1, -1.0)
    t.testing.assert_close(actual, expected)
    print("All tests in `test_pad2d_multi_channel` passed!")


def test_conv2d(conv2d, n_tests=5):
    import numpy as np

    for i in range(n_tests):
        b = np.random.randint(1, 10)
        h = np.random.randint(10, 300)
        w = np.random.randint(10, 300)
        ci = np.random.randint(1, 20)
        co = np.random.randint(1, 20)
        stride = tuple(np.random.randint(1, 5, size=(2,)))
        padding = tuple(np.random.randint(0, 5, size=(2,)))
        kernel_size = tuple(np.random.randint(1, 10, size=(2,)))
        x = t.randn((b, ci, h, w), dtype=t.float64)
        weights = t.randn((co, ci, *kernel_size), dtype=t.float64)
        my_output = conv2d(x, weights, stride=stride, padding=padding)
        torch_output = t.conv2d(x, weights, stride=stride, padding=padding)
        t.testing.assert_close(my_output, torch_output)
    print("All tests in `test_conv2d` passed!")


def test_maxpool2d(my_maxpool2d, n_tests=20):
    import numpy as np

    for i in range(n_tests):
        b = np.random.randint(1, 10)
        h = np.random.randint(10, 50)
        w = np.random.randint(10, 50)
        ci = np.random.randint(1, 20)
        stride = (
            None
            if np.random.random() < 0.5
            else tuple(np.random.randint(1, 5, size=(2,)))
        )
        kernel_size = tuple(np.random.randint(1, 10, size=(2,)))
        kH, kW = kernel_size
        padding = np.random.randint(0, 1 + kH // 2), np.random.randint(0, 1 + kW // 2)
        x = t.randn((b, ci, h, w))
        my_output = my_maxpool2d(
            x,
            kernel_size,
            stride=stride,
            padding=padding,
        )
        torch_output = t.max_pool2d(
            x,
            kernel_size,
            stride=stride,  # type: ignore (None actually is allowed)
            padding=padding,
        )
        t.testing.assert_close(my_output, torch_output)
    print("All tests in `test_maxpool2d` passed!")


def test_maxpool2d_module(MaxPool2d, n_tests=20, tuples=False):
    import numpy as np

    for i in range(n_tests):
        b = np.random.randint(1, 10)
        h = np.random.randint(10, 50)
        w = np.random.randint(10, 50)
        ci = np.random.randint(1, 20)
        if tuples:
            stride = (
                None
                if np.random.random() < 0.5
                else tuple(np.random.randint(1, 5, size=(2,)))
            )
            kernel_size = tuple(np.random.randint(1, 10, size=(2,)))
            kH, kW = kernel_size
            padding = np.random.randint(0, 1 + kH // 2), np.random.randint(
                0, 1 + kW // 2
            )
        else:
            stride = None if np.random.random() < 0.5 else np.random.randint(1, 5)
            kernel_size = np.random.randint(1, 10)
            padding = np.random.randint(0, 1 + kernel_size // 2)
        x = t.randn((b, ci, h, w))
        my_output = MaxPool2d(
            kernel_size,
            stride=stride,
            padding=padding,
        )(x)

        torch_output = nn.MaxPool2d(
            kernel_size,
            stride=stride,
            padding=padding,
        )(x)
        t.testing.assert_close(my_output, torch_output)
    print("All tests in `test_maxpool2d_module` passed!")


def compare_module_attributes(custom_module, reference_module):
    module_name = custom_module.__class__.__name__

    # Compare named parameters
    custom_params = dict(custom_module.named_parameters())
    ref_params = dict(reference_module.named_parameters())

    assert list(custom_params.keys()) == list(ref_params.keys()), (
        f"Your {module_name} should declare the following parameters in order.\n"
        f"Expected: {list(ref_params.keys())}\nActual: {list(custom_params.keys())}"
    )

    # Check tensor shapes for parameters
    for name in custom_params:
        if hasattr(custom_params[name], "shape") and hasattr(ref_params[name], "shape"):
            assert custom_params[name].shape == ref_params[name].shape, (
                f"Shape mismatch for parameter '{name}' in {module_name}.\n"
                f"Expected shape: {ref_params[name].shape}, Actual shape: {custom_params[name].shape}"
            )

    # Compare named buffers
    custom_buffers = dict(custom_module.named_buffers())
    ref_buffers = dict(reference_module.named_buffers())

    assert list(custom_buffers.keys()) == list(ref_buffers.keys()), (
        f"Your {module_name} should declare the following buffers in order.\n"
        f"Expected: {list(ref_buffers.keys())}\nActual: {list(custom_buffers.keys())}"
    )

    # Check tensor shapes for buffers
    for name in custom_buffers:
        if hasattr(custom_buffers[name], "shape") and hasattr(
            ref_buffers[name], "shape"
        ):
            assert custom_buffers[name].shape == ref_buffers[name].shape, (
                f"Shape mismatch for buffer '{name}' in {module_name}.\n"
                f"Expected shape: {ref_buffers[name].shape}, Actual shape: {custom_buffers[name].shape}"
            )


def test_conv2d_module(Conv2d, n_tests=5, tuples=False):
    """
    Your weight should be called 'weight' and have an appropriate number of elements.
    """
    m = Conv2d(4, 5, 3)
    assert isinstance(
        m.weight, t.nn.parameter.Parameter
    ), "Weight should be registered a parameter!"
    assert m.weight.nelement() == 4 * 5 * 3 * 3
    m_sol = nn.Conv2d(4, 5, 3, bias=False)
    compare_module_attributes(m, m_sol)
    import numpy as np

    for i in range(n_tests):
        b = np.random.randint(1, 10)
        h = np.random.randint(10, 300)
        w = np.random.randint(10, 300)
        ci = np.random.randint(1, 20)
        co = np.random.randint(1, 20)
        if tuples:
            stride = tuple(np.random.randint(1, 5, size=(2,)))
            padding = tuple(np.random.randint(0, 5, size=(2,)))
            kernel_size = tuple(np.random.randint(1, 10, size=(2,)))
        else:
            stride = np.random.randint(1, 5)
            padding = np.random.randint(0, 5)
            kernel_size = np.random.randint(1, 10)
        x = t.randn((b, ci, h, w))
        my_conv = Conv2d(
            in_channels=ci,
            out_channels=co,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
        )
        my_output = my_conv(x)
        torch_output = t.conv2d(x, my_conv.weight, stride=stride, padding=padding)
        t.testing.assert_close(my_output, torch_output)
    print("All tests in `test_conv2d_module` passed!")


def test_relu(ReLU):
    x = t.randn(10) - 0.5
    actual = ReLU()(x)
    expected = F.relu(x)
    t.testing.assert_close(actual, expected)
    print("All tests in `test_relu` passed!")


def test_flatten(Flatten):
    x = t.arange(24).reshape((2, 3, 4))
    assert Flatten(start_dim=0)(x).shape == (24,)
    assert Flatten(start_dim=1)(x).shape == (2, 12)
    assert Flatten(start_dim=0, end_dim=1)(x).shape == (6, 4)
    assert Flatten(start_dim=0, end_dim=-2)(x).shape == (6, 4)
    print("All tests in `test_flatten` passed!")


def test_linear_forward(Linear, bias=False):
    """Your Linear should produce identical results to torch.nn given identical parameters."""
    x = t.rand((10, 512))
    yours = Linear(512, 64, bias=bias)
    official = t.nn.Linear(512, 64, bias=bias)
    # ensure the weights are the same
    yours.load_state_dict(official.state_dict())
    actual = yours(x)
    expected = official(x)
    t.testing.assert_close(actual, expected)
    print("All tests in `test_linear_forward` passed!")


def test_linear_parameters(Linear, bias=False):
    l = Linear(2, 3, bias=bias)
    l_sol = nn.Linear(2, 3, bias=bias)
    compare_module_attributes(l, l_sol)
    if not bias:
        assert l.bias is None, "Bias should be None when not enabled."
    print("All tests in `test_linear_parameters` passed!")


def test_linear_no_bias(Linear):
    import part2_cnns.solutions as solutions

    x = t.rand((10, 512))
    yours = Linear(512, 64, bias=False)

    assert yours.bias is None, "Bias should be None when not enabled."
    assert len(list(yours.parameters())) == 1

    official = solutions.Linear(512, 64, bias=False)
    yours.weight = official.weight
    actual = yours(x)
    expected = official(x)
    t.testing.assert_close(actual, expected)
    print("All tests in `test_linear_no_bias` passed!")


def test_mlp_module(SimpleMLP):
    import part2_cnns.solutions as solutions

    mlp: nn.Module = SimpleMLP()
    num_params = sum(p.numel() for p in mlp.parameters())
    assert (
        num_params == 79510
    ), f"Expected (28*28 + 1) * 100 + ((100 + 1) * 10) = 79510 parameters, got {num_params}"
    mlp_sol = solutions.SimpleMLP()
    compare_module_attributes(mlp, mlp_sol)
    print("All tests in `test_mlp_module` passed!")


def test_mlp_forward(SimpleMLP):
    import part2_cnns.solutions as solutions

    mlp: nn.Module = SimpleMLP()
    mlp_sol = solutions.SimpleMLP()
    x = t.rand((10, 28, 28))

    # ensure the weights are the same
    mlp.load_state_dict(mlp_sol.state_dict())

    out = mlp(x)
    out_sol = mlp_sol(x)
    t.testing.assert_close(out, out_sol)
    print("All tests in `test_mlp_forward` passed!")


def test_batchnorm2d_module(BatchNorm2d):
    import part2_cnns.solutions as solutions

    """The public API of the module should be the same as the real PyTorch version."""
    num_features = 2
    bn = BatchNorm2d(num_features)
    assert bn.num_features == num_features
    assert isinstance(
        bn.weight, t.nn.parameter.Parameter
    ), f"weight has wrong type: {type(bn.weight)}"
    assert isinstance(
        bn.bias, t.nn.parameter.Parameter
    ), f"bias has wrong type: {type(bn.bias)}"
    assert isinstance(
        bn.running_mean, t.Tensor
    ), f"running_mean has wrong type: {type(bn.running_mean)}"
    assert isinstance(
        bn.running_var, t.Tensor
    ), f"running_var has wrong type: {type(bn.running_var)}"
    assert isinstance(
        bn.num_batches_tracked, t.Tensor
    ), f"num_batches_tracked has wrong type: {type(bn.num_batches_tracked)}"
    bn_sol = solutions.BatchNorm2d(num_features)
    compare_module_attributes(bn, bn_sol)
    print("All tests in `test_batchnorm2d_module` passed!")


def test_batchnorm2d_forward(BatchNorm2d):
    """For each channel, mean should be very close to 0 and std kinda close to 1 (because of eps)."""
    num_features = 2
    bn = BatchNorm2d(num_features)
    assert bn.training
    x = t.randn((100, num_features, 3, 4))
    out = bn(x)
    assert x.shape == out.shape
    t.testing.assert_close(out.mean(dim=(0, 2, 3)), t.zeros(num_features))
    t.testing.assert_close(
        out.std(dim=(0, 2, 3)), t.ones(num_features), atol=1e-3, rtol=1e-3
    )
    print("All tests in `test_batchnorm2d_forward` passed!")


def test_batchnorm2d_running_mean(BatchNorm2d):
    """Over repeated forward calls with the same data in train mode, the running mean should converge to the actual mean."""
    bn = BatchNorm2d(3, momentum=0.6)
    assert bn.training
    x = t.arange(12).float().view((2, 3, 2, 1))
    mean = t.tensor([3.5000, 5.5000, 7.5000])
    num_batches = 30
    for i in range(num_batches):
        bn(x)
        expected_mean = (1 - ((1 - bn.momentum) ** (i + 1))) * mean
        t.testing.assert_close(bn.running_mean, expected_mean)
    assert bn.num_batches_tracked.item() == num_batches

    # Large enough momentum and num_batches -> running_mean should be very close to actual mean
    bn.eval()
    actual_eval_mean = bn(x).mean((0, 2, 3))
    t.testing.assert_close(actual_eval_mean, t.zeros(3))
    print("All tests in `test_batchnorm2d_running_mean` passed!")


def test_averagepool(AveragePool):
    x = t.arange(24).reshape((1, 2, 3, 4)).float()
    actual = AveragePool()(x)
    expected = t.tensor([[5.5, 17.5]])
    t.testing.assert_close(actual, expected)
    print("All tests in `test_averagepool` passed!")


def test_residual_block(ResidualBlock):
    """
    Test the user's implementation of `ResidualBlock`.
    """
    import part2_cnns.solutions as solutions

    # Create random input tensor
    x = t.randn(1, 3, 64, 64)

    # Instantiate both user and reference models
    user_model = ResidualBlock(in_feats=3, out_feats=3)
    ref_model = solutions.ResidualBlock(in_feats=3, out_feats=3)
    # Check parameter count
    user_params = sum(p.numel() for p in user_model.parameters())
    ref_params = sum(p.numel() for p in ref_model.parameters())
    # Special case this scenario because occasionally people will
    # unconditionally create the right-hand branch and then only conditionally
    # switch in the forward method whether to use the right-hand branch or not.
    if user_params > ref_params:
        error_message = f"""
        When the first_stride=1, there are more parameters ({user_params}) than
        expected ({ref_params}). Make sure that you don't create unnecessary
        convolutions for the right-hand branch when first_stride=1. That is your
        initialization code should only initialize the right-hand branch when
        first_stride is not 1.
        """
        raise AssertionError(error_message)
    assert (
        user_params == ref_params
    ), f"Parameter count mismatch (when first_stride=1). Expected {ref_params}, got {user_params}."
    # Check forward function output is correct shape
    user_output = user_model(x)
    assert user_output.shape == (
        1,
        3,
        64,
        64,
    ), f"Incorrect shape, expected (batch=1, out_feats=4, height=64, width=64), got {user_output.shape}"
    print("Passed all tests when first_stride=1")

    # Same checks, but now with nontrivial stride
    user_model = ResidualBlock(in_feats=3, out_feats=4, first_stride=2)
    ref_model = solutions.ResidualBlock(in_feats=3, out_feats=4, first_stride=2)
    user_params = sum(p.numel() for p in user_model.parameters())
    ref_params = sum(p.numel() for p in ref_model.parameters())
    assert (
        user_params == ref_params
    ), f"Parameter count mismatch (when first_stride>1). Expected {ref_params}, got {user_params}."
    user_output = user_model(x)
    assert user_output.shape == (
        1,
        4,
        32,
        32,
    ), f"Incorrect shape, expected (batch=1, out_feats=4, height/first_stride=32, width/first_stride=32), got {user_output.shape}"
    print("Passed all tests when first_stride>1")

    print("All tests in `test_residual_block` passed!")


def test_block_group(BlockGroup):
    """
    Test the user's implementation of `ResidualBlock`.
    """
    import part2_cnns.solutions as solutions

    # Create random input tensor
    x = t.randn(1, 3, 64, 64)

    # Instantiate both user and reference models
    user_model = BlockGroup(n_blocks=2, in_feats=3, out_feats=3)
    ref_model = solutions.BlockGroup(n_blocks=2, in_feats=3, out_feats=3)
    # Check parameter count
    user_params = sum(p.numel() for p in user_model.parameters())
    ref_params = sum(p.numel() for p in ref_model.parameters())
    assert (
        user_params == ref_params
    ), "Parameter count mismatch (when n_blocks=2, first_stride=1)"
    # Check forward function output is correct shape
    user_output = user_model(x)
    assert user_output.shape == (
        1,
        3,
        64,
        64,
    ), f"Incorrect shape, expected (batch=1, out_feats=4, height=64, width=64), got {user_output.shape}"
    print("Passed all tests when first_stride=1")

    # Same checks, but now with nontrivial stride
    user_model = BlockGroup(n_blocks=2, in_feats=3, out_feats=4, first_stride=2)
    ref_model = solutions.BlockGroup(
        n_blocks=2, in_feats=3, out_feats=4, first_stride=2
    )
    user_params = sum(p.numel() for p in user_model.parameters())
    ref_params = sum(p.numel() for p in ref_model.parameters())
    assert (
        user_params == ref_params
    ), "Parameter count mismatch (when n_blocks=2, first_stride>1)"
    user_output = user_model(x)
    assert user_output.shape == (
        1,
        4,
        32,
        32,
    ), f"Incorrect shape, expected (batch=1, out_feats=4, height/first_stride=32, width/first_stride=32), got {user_output.shape}"
    print("Passed all tests when first_stride>1")

    # Same checks, but now with a larger n_blocks
    user_model = BlockGroup(n_blocks=5, in_feats=3, out_feats=4, first_stride=2)
    ref_model = solutions.BlockGroup(
        n_blocks=5, in_feats=3, out_feats=4, first_stride=2
    )
    user_params = sum(p.numel() for p in user_model.parameters())
    ref_params = sum(p.numel() for p in ref_model.parameters())
    assert (
        user_params == ref_params
    ), "Parameter count mismatch (when n_blocks=5, first_stride>1)"
    user_output = user_model(x)
    assert user_output.shape == (
        1,
        4,
        32,
        32,
    ), f"Incorrect shape, expected (batch=1, out_feats=4, height/first_stride=32, width/first_stride=32), got {user_output.shape}"
    print("Passed all tests when n_blocks>2")

    print("All tests in `test_block_group` passed!")


def test_get_resnet_for_feature_extraction(get_resnet_for_feature_extraction):
    resnet: nn.Module = get_resnet_for_feature_extraction(10)

    num_params = len(list(resnet.parameters()))

    error_msg = "\nNote - make sure you've defined your resnet modules in the correct order (with the final linear layer last), \
otherwise this can cause issues for the test function."

    # Check all gradients are correct
    for i, (name, param) in enumerate(resnet.named_parameters()):
        if i < num_params - 2:
            assert not param.requires_grad, (
                f"Found param {name!r} before the final layer, which has requires_grad=True."
                + error_msg
            )
        else:
            assert param.requires_grad, (
                f"Found param {name!r} in the final layer, which has requires_grad=False."
                + error_msg
            )
            if param.ndim == 2:
                assert tuple(param.shape) == (10, 512), (
                    f"Expected final linear layer weights to have shape (n_classes=10, 512), instead found {tuple(param.shape)}"
                    + error_msg
                )
            else:
                assert tuple(param.shape) == (10,), (
                    f"Expected final linear layer bias to have shape (n_classes=10,), instead found {tuple(param.shape)}"
                    + error_msg
                )

    print("All tests in `test_get_resnet_for_feature_extraction` passed!")
