import json
import math
from typing import TYPE_CHECKING, Any, Literal

import einops
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import torch as t
from jaxtyping import Float
from matplotlib import pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
from openai import OpenAIError
from plotly.subplots import make_subplots
from tabulate import tabulate
from torch import Tensor
from tqdm import tqdm

# ============= PART 3.1 UTILS =============

Arr = np.ndarray

if TYPE_CHECKING:
    from part31_superposition_and_saes.solutions import Model

red = plt.get_cmap("coolwarm")(0.0)
blue = plt.get_cmap("coolwarm")(1.0)
light_grey = np.array([15 / 16, 15 / 16, 15 / 16, 1.0])
red_grey_blue_cmap = LinearSegmentedColormap.from_list(
    "modified_coolwarm",
    np.vstack([np.linspace(red, light_grey, 128), np.linspace(light_grey, blue, 128)]),
)


def cast_element_to_nested_list(elem, shape: tuple):
    """
    Creates a nested list of shape `shape`, where every element is `elem`.
    Example: ("a", (2, 2)) -> [["a", "a"], ["a", "a"]]
    """
    if len(shape) == 0:
        return elem
    return [cast_element_to_nested_list(elem, shape[1:])] * shape[0]


def plot_correlated_features(batch: Float[Tensor, "batch feats"], title: str):
    go.Figure(
        data=[
            go.Bar(y=batch.squeeze()[:, 0].tolist(), name="Feature 0"),
            go.Bar(y=batch.squeeze()[:, 1].tolist(), name="Feature 1"),
        ],
        layout=dict(
            template="simple_white",
            title=title,
            bargap=0.4,
            bargroupgap=0.0,
            xaxis=dict(tickmode="array", tickvals=list(range(batch.squeeze().shape[0]))),
            xaxis_title="Pairs of features",
            yaxis_title="Feature Values",
            height=400,
            width=1000,
        ),
    ).show()


def sort_W_by_monosemanticity(
    W: Float[Tensor, "feats d_hidden"],
) -> tuple[Float[Tensor, "feats d_hidden"], int]:
    """
    Rearranges the columns of the tensor (i.e. rearranges neurons) in descending order of
    their monosemanticity (where we define monosemanticity as the largest fraction of this
    neuron's norm which is a single feature).

    Also returns the number of "monosemantic features", which we (somewhat arbitrarily)
    define as the fraction being >90% of the total norm.
    """
    norm_by_neuron = W.pow(2).sum(dim=0)
    monosemanticity = W.abs().max(dim=0).values / (norm_by_neuron + 1e-6).sqrt()

    column_order = monosemanticity.argsort(descending=True).tolist()

    n_monosemantic_features = int((monosemanticity.abs() > 0.99).sum().item())

    return W[:, column_order], n_monosemantic_features


def rearrange_full_tensor(
    W: Float[Tensor, "inst d_hidden feats"],
):
    """
    Same as above, but works on W in its original form, and returns a list of
    number of monosemantic features per instance.
    """
    n_monosemantic_features_list = []

    for i, W_inst in enumerate(W):
        W_inst_rearranged, n_monosemantic_features = sort_W_by_monosemanticity(W_inst.T)
        W[i] = W_inst_rearranged.T
        n_monosemantic_features_list.append(n_monosemantic_features)

    return W, n_monosemantic_features_list


def get_viridis_str(v: float) -> str:
    r, g, b, a = plt.get_cmap("viridis")(v)
    r, g, b = int(r * 255), int(g * 255), int(b * 255)
    return f"rgb({r}, {g}, {b})"


def get_viridis(v: float) -> tuple[float, float, float]:
    r, g, b, a = plt.get_cmap("viridis")(v)
    return (r, g, b)


def clamp(x: float, min_val: float, max_val: float) -> float:
    return min(max(x, min_val), max_val)


def plot_features_in_Nd(
    W: Float[Tensor, "inst d_hidden feats"],
    height: int,
    width: int,
    title: str | None = None,
    subplot_titles: list[str] | None = None,
    neuron_plot: bool = False,
):
    n_instances, d_hidden, n_feats = W.shape

    W = W.detach().cpu()

    # Rearrange to align with standard basis
    W, n_monosemantic_features = rearrange_full_tensor(W)

    # Normalize W, i.e. W_normed[inst, i] is normalized i-th feature vector
    W_normed = W / (1e-6 + t.linalg.norm(W, 2, dim=1, keepdim=True))

    # We get interference[i, j] = sum_{j!=i} (W_normed[i] @ W[j]) (ignoring the instance dimension)
    # because then we can calculate superposition by squaring & summing this over j
    interference = einops.einsum(
        W_normed,
        W,
        "instances hidden feats_i, instances hidden feats_j -> instances feats_i feats_j",
    )
    interference[:, range(n_feats), range(n_feats)] = 0

    # Now take the sum, and sqrt (we could just as well not sqrt)
    # Heuristic: polysemanticity is zero if it's orthogonal to all else, one if it's perfectly aligned with any other single vector
    polysemanticity = einops.reduce(
        interference.pow(2),
        "instances feats_i feats_j -> instances feats_i",
        "sum",
    ).sqrt()
    colors = [
        [get_viridis_str(v.item()) for v in polysemanticity_for_this_instance]
        for polysemanticity_for_this_instance in polysemanticity
    ]

    # Get the norms (this is the bar height)
    W_norms = einops.reduce(
        W.pow(2),
        "instances hidden feats -> instances feats",
        "sum",
    ).sqrt()

    # We need W.T @ W for the heatmap (unless this is a neuron plot, then we just use w)
    if not (neuron_plot):
        WtW = einops.einsum(
            W,
            W,
            "instances hidden feats_i, instances hidden feats_j -> instances feats_i feats_j",
        )
        imshow_data = WtW.numpy()
    else:
        imshow_data = einops.rearrange(W, "instances hidden feats -> instances feats hidden").numpy()

    # Get titles (if they exist). Make sure titles only apply to the bar chart in each row
    titles = ["Heatmap of " + ("W" if neuron_plot else "W<sup>T</sup>W")] * n_instances + [
        "Neuron weights<br>stacked bar plot" if neuron_plot else "Feature norms"
    ] * n_instances  # , ||W<sub>i</sub>||
    if subplot_titles is not None:
        for i, st in enumerate(subplot_titles):
            titles[i] = st + "<br>" + titles[i]

    total_height = 0.9 if title is None else 0.8
    if neuron_plot:
        heatmap_height_fraction = clamp(n_feats / (d_hidden + n_feats), 0.5, 0.75)
    else:
        heatmap_height_fraction = 1 - clamp(n_feats / (30 + n_feats), 0.5, 0.75)
    row_heights = [
        total_height * heatmap_height_fraction,
        total_height * (1 - heatmap_height_fraction),
    ]

    n_rows = 2
    n_cols = n_instances

    fig = make_subplots(
        rows=n_rows,
        cols=n_cols,
        vertical_spacing=0.1 if neuron_plot else 0.05,
        row_heights=row_heights,
        subplot_titles=titles,
    )
    for inst in range(n_instances):
        # (1) Add bar charts
        # If it's the non-neuron plot then x = features, y = norms of those features. If it's the
        # neuron plot, our x = neurons (d_hidden), y = the loadings of features on those neurons. In
        # both cases, colors = polysemanticity of features, which we've already computed
        if neuron_plot:
            for feat in range(n_feats):
                fig.add_trace(
                    go.Bar(
                        x=t.arange(d_hidden),
                        y=W[inst, :, feat],
                        marker=dict(color=[colors[inst][feat]] * d_hidden),
                        width=0.9,
                    ),
                    col=1 + inst,
                    row=2,
                )
        else:
            fig.add_trace(
                go.Bar(
                    y=t.arange(n_feats).flip(0),
                    x=W_norms[inst],
                    marker=dict(color=colors[inst]),
                    width=0.9,
                    orientation="h",
                ),
                col=1 + inst,
                row=2,
            )
        # (2) Add heatmap
        # Code is same for neuron plot vs no neuron plot, although data is different: W.T @ W vs W
        fig.add_trace(
            go.Image(
                z=red_grey_blue_cmap((1 + imshow_data[inst]) / 2, bytes=True),
                colormodel="rgba256",
                customdata=imshow_data[inst],
                hovertemplate="""In: %{x}<br>\nOut: %{y}<br>\nWeight: %{customdata:0.2f}""",
            ),
            col=1 + inst,
            row=1,
        )

    if neuron_plot:
        # Stacked plots to allow for all features to be seen
        fig.update_layout(barmode="relative")

        # Weird naming convention for subplots, make sure we have a list of the subplot names for bar charts so we can iterate through them
        n0 = 1 + n_instances
        fig_indices = [str(i) if i != 1 else "" for i in range(n0, n0 + n_instances)]

        for inst in range(n_instances):
            fig["layout"][f"yaxis{fig_indices[inst]}_range"] = [-6, 6]  # type: ignore

            # Add the background colors
            row, col = (2, 1 + inst)
            fig.add_vrect(
                x0=-0.5,
                x1=-0.5 + n_monosemantic_features[inst],
                fillcolor="#440154",
                line_width=0.0,
                opacity=0.2,
                col=col,  # type: ignore
                row=row,  # type: ignore
                layer="below",
            )
            fig.add_vrect(
                x0=-0.5 + n_monosemantic_features[inst],
                x1=-0.5 + d_hidden,
                fillcolor="#fde725",
                line_width=0.0,
                opacity=0.2,
                col=col,  # type: ignore
                row=row,  # type: ignore
                layer="below",
            )

    else:
        # Add annotation of "features" on the y-axis of the bar plot
        fig_indices = [str(i) if i != 1 else "" for i in range(n_instances + 1, 2 * n_instances + 1)]
        for inst in range(n_instances):
            fig.add_annotation(
                text="Features âž”",  # âž¤â†’â®•ðŸ¡’âžœ
                xref=f"x{fig_indices[inst]} domain",
                yref=f"y{fig_indices[inst]} domain",
                x=-0.13,
                y=0.99,  # Positioning the annotation outside the first bar plot subfigure
                showarrow=False,
                font=dict(size=12),
                textangle=90,  # Set the text angle to 90 degrees for vertical text
            )

    # Add a horizontal line at the point where n_features = d_hidden (in non-neuron plot). After this point,
    # we must have superposition if we represent all features.
    for annotation in fig.layout.annotations:
        annotation.font.size = 13
    if not neuron_plot:
        fig.add_hline(
            y=n_feats - d_hidden - 0.5,
            line=dict(width=0.5),
            opacity=1.0,
            row=2,  # type: ignore
            annotation_text=f" d_hidden={d_hidden}",
            annotation_position="bottom left",  # "bottom"
            annotation_font_size=11,
        )

    # fig.update_traces(marker_size=1)
    fig.update_layout(
        showlegend=False,
        width=width,
        height=height,
        margin=dict(t=40 if title is None else 110, b=40, l=50, r=40),
        plot_bgcolor="#eee",
        title=title,
        title_y=0.95,
        # template="simple_white",
    )

    fig.update_xaxes(showticklabels=False, showgrid=False)  # visible=False
    fig.update_yaxes(showticklabels=False, showgrid=False)

    fig.show()


def plot_features_in_Nd_discrete(
    W1: Float[Tensor, "inst d_hidden feats"],
    W2: Float[Tensor, "inst feats d_hidden"],
    legend_names: list[str],
    height: int = 600,
    width: int | None = None,
    title: str | None = None,
):
    n_instances, d_hidden, n_feats = W1.shape

    if width is None:
        width = 200 * (n_instances + 1)

    color_list = px.colors.qualitative.D3 + px.colors.qualitative.T10
    assert n_feats <= len(color_list), "Too many features for discrete plot"

    W1 = W1.detach().cpu()
    W2 = W2.detach().cpu()

    titles = [f"Inst={inst}<br>W<sub>1</sub>" for inst in range(n_instances)] + [
        "W<sub>2</sub>" for inst in range(n_instances)
    ]

    fig = make_subplots(rows=2, cols=n_instances, subplot_titles=titles, vertical_spacing=0.1)
    for inst in range(n_instances):
        for feat in range(n_feats):
            fig.add_trace(
                go.Bar(
                    x=t.arange(d_hidden),
                    y=W1[inst, :, feat],
                    marker=dict(color=[color_list[feat]] * d_hidden),
                    showlegend=inst == 0,
                    name=legend_names[feat],
                    width=0.9,
                ),
                col=1 + inst,
                row=1,
            )
            # showlegend=inst==0, name=legend_names[feat]
            fig.add_trace(
                go.Bar(
                    x=t.arange(d_hidden),
                    y=W2[inst, feat, :],
                    marker=dict(color=[color_list[feat]] * d_hidden),
                    showlegend=False,
                    width=0.9,
                ),
                col=1 + inst,
                row=2,
            )

    # Stacked plots to allow for all features to be seen
    fig.update_layout(barmode="relative")

    # Weird naming convention for subplots, make sure we have a list of the subplot names for bar charts so we can iterate through them
    fig_indices = [str(i) if i != 1 else "" for i in range(1, 1 + 2 * n_instances)]
    m = max(W1.abs().max().item(), W2.abs().max().item())
    for inst in range(2 * n_instances):
        fig["layout"][f"yaxis{fig_indices[inst]}_range"] = [-m - 1, m + 1]  # type: ignore

    fig.update_layout(
        legend_title_text="Feature importances",
        width=width,
        height=height,
        margin=dict(t=40 if title is None else 110, b=40, l=50, r=40),
        plot_bgcolor="#eee",
        title=title,
        title_y=0.95,
    )

    fig.update_xaxes(showticklabels=False, showgrid=False)
    fig.update_yaxes(showgrid=False)
    fig.show()


def plot_features_in_2d(
    W: Float[Tensor, "*inst d_hidden feats"] | list[Float[Tensor, "d_hidden feats"]],
    colors: Float[Tensor, "inst feats"] | list[str] | list[list[str]] | None = None,
    title: str | None = None,
    subplot_titles: list[str] | None = None,
    allow_different_limits_across_subplots: bool = False,
    n_rows: int | None = None,
):
    """
    Visualises superposition in 2D.

    If values is 4D, the first dimension is assumed to be timesteps, and an animation is created.
    """
    # Convert W into a list of 2D tensors, each of shape [feats, d_hidden=2]
    if isinstance(W, Tensor):
        if W.ndim == 2:
            W = W.unsqueeze(0)
        n_instances, d_hidden, n_feats = W.shape
        n_feats_list = []
        W = W.detach().cpu()
    else:
        # Hacky case which helps us deal with double descent exercises (this is never used outside of those exercises)
        assert all(w.ndim == 2 for w in W)
        n_feats_list = [w.shape[1] for w in W]
        n_feats = max(n_feats_list)
        n_instances = len(W)
        W = [w.detach().cpu() for w in W]

    W_list: list[Tensor] = [W_instance.T for W_instance in W]

    # Get some plot characteristics
    limits_per_instance = (
        [w.abs().max() * 1.1 for w in W_list]
        if allow_different_limits_across_subplots
        else [1.5 for _ in range(n_instances)]
    )
    linewidth, markersize = (1, 4) if (n_feats >= 25) else (1.5, 6)

    # Maybe break onto multiple rows
    if n_rows is None:
        n_rows, n_cols = 1, n_instances
        row_col_tuples = [(0, i) for i in range(n_instances)]
    else:
        n_cols = n_instances // n_rows
        row_col_tuples = [(i // n_cols, i % n_cols) for i in range(n_instances)]

    # Convert colors into a 2D list of strings, with shape [instances, feats]
    if colors is None:
        colors_list = cast_element_to_nested_list("black", (n_instances, n_feats))
    elif isinstance(colors, str):
        colors_list = cast_element_to_nested_list(colors, (n_instances, n_feats))
    elif isinstance(colors, list):
        # List of strings -> same for each instance and feature
        if isinstance(colors[0], str):
            assert len(colors) == n_feats
            colors_list = [colors for _ in range(n_instances)]
        # List of lists of strings -> different across instances & features (we broadcast)
        else:
            colors_list = []
            for i, colors_for_instance in enumerate(colors):
                assert len(colors_for_instance) in (1, n_feats_list[i])
                colors_list.append(colors_for_instance * (n_feats_list[i] if len(colors_for_instance) == 1 else 1))
    elif isinstance(colors, Tensor):
        assert colors.shape == (n_instances, n_feats)
        colors_list = [[get_viridis(v) for v in color] for color in colors.tolist()]

    # Create a figure and axes, and make sure axs is a 2D array
    fig, axs = plt.subplots(n_rows, n_cols, figsize=(2.5 * n_cols, 2.5 * n_rows))
    axs = np.broadcast_to(axs, (n_rows, n_cols))

    # If there are titles, add more spacing for them
    fig.subplots_adjust(bottom=0.2, top=(0.8 if title else 0.9), left=0.1, right=0.9, hspace=0.5)

    # Initialize lines and markers
    for instance_idx, ((row, col), limits_per_instance) in enumerate(zip(row_col_tuples, limits_per_instance)):
        # Get the right axis, and set the limits
        ax = axs[row, col]
        ax.set_xlim(-limits_per_instance, limits_per_instance)
        ax.set_ylim(-limits_per_instance, limits_per_instance)
        ax.set_aspect("equal", adjustable="box")

        # Add all the features for this instance
        _n_feats = n_feats if len(n_feats_list) == 0 else n_feats_list[instance_idx]
        for feature_idx in range(_n_feats):
            x, y = W_list[instance_idx][feature_idx].tolist()
            color = colors_list[instance_idx][feature_idx]
            ax.plot([0, x], [0, y], color=color, lw=linewidth)[0]
            ax.plot([x, x], [y, y], color=color, marker="o", markersize=markersize)[0]

        # Add titles & subtitles
        if title:
            fig.suptitle(title, fontsize=15)
        if subplot_titles:
            axs[row, col].set_title(subplot_titles[instance_idx], fontsize=12)

    plt.show()


def animate_features_in_2d(
    data_log: list[dict[str, Any]] | dict[str, Any],
    rows: list[str],
    instances: list[int] | None = None,
    color_hidden_activations_by_loss: bool = False,
    box_size: int = 250,
    fps: int = 50,
    color_resampled_latents: bool = False,
    n_seconds_to_highlight: float = 0.5,
    highlight_threshold: float = 0.3,
    title: str = "SAE trained on toy model",
    filename: str | None = None,
) -> str | None:
    """
    Creates an animation of 2D SAE features, as they are learned by the model.

    Args:
        data_log:       List of dicts returned from `sae.optimize`, or a single dict (e.g. representing an end state).
                        This will contain keys like "W_enc" (model weights), "steps" (list of step numbers), "loss"
                        (list of the tensor loss values over time), "h" (model hidden activations), etc.
        rows:           List of strings, each of which is a title for a row in the animation. For example, "W_enc" means
                        we plot the encoder weights, and "h_recon" means we plot the reconstructed hidden activations.
        instances:      If supplied, we only plot a subset of instances.
    """
    ZERO_THRESHOLD = 1e-3

    # TODO - maybe it's okay to specify "W_mag" when it doesn't already exist? Although kinda bad practice to have that here?
    # Check all the rows we're trying to plot are valid
    invalid_rows = [row for row in rows if row not in ["W_enc", "W_gate", "W_mag", "_W_dec", "h", "h_r"]]
    assert len(invalid_rows) == 0, f"Invalid row specified: {invalid_rows}"
    all_rows = sorted(data_log[0].keys())
    missing_rows = [row for row in rows if row not in all_rows]
    assert len(missing_rows) == 0, f"Missing rows: {missing_rows}"

    # Get data from data_log
    W_name = "W_enc" if "W_enc" in data_log[0] else "W_gate"
    W_ts = t.stack([data_log[i][W_name] for i in range(len(data_log))])
    n_timesteps, n_inst_all, d_in, d_sae = W_ts.shape
    assert d_in == 2, "Only 2D data is supported"
    batch_size = data_log[0]["h"].shape[0]

    # Get args into the right format, and define useful stuff
    data_log_list = [data_log] if isinstance(data_log, dict) else data_log
    instances_list = instances or list(range(n_inst_all))
    n_inst = len(instances_list)
    n_steps_to_highlight = int(n_seconds_to_highlight * fps)

    # Some helper functions
    def rearrange_fn(tensor: Tensor, row: str) -> Tensor:
        return (
            tensor
            if row in ["W_enc", "W_gate", "W_mag"]
            else {
                "h": lambda x: einops.rearrange(x, "batch inst d_in -> inst d_in batch"),
                "h_r": lambda x: einops.rearrange(x, "batch inst d_in -> inst d_in batch"),
                "_W_dec": lambda x: x.transpose(-1, -2),
            }[row](tensor)
        )

    def html_row_name(row: str, inst: int):
        name = {
            # f"W<sub>{row.split('_')[-1]}</sub>" if "W_" in row else
            "W_enc": "Encoder weights",
            "W_gate": "Gating weights",
            "W_mag": "Magnitude weights",
            "_W_dec": "Decoder weights",
            "h": "Hidden states",
            "h_r": "Hidden states, reconstructed",
        }[row]
        if (n_inst > 1) and (row == rows[0]):
            name += f" (inst {inst})"
        return name

    # We have 2 types of rows: weights and hidden activations
    def get_row_type(row: str) -> str:
        return "W" if "W_" in row else "h"

    # W_row_types = [r for r in all_rows if get_row_type(r) == "W"]
    h_row_types = [r for r in all_rows if get_row_type(r) == "h"]
    present_row_types = sorted(set([get_row_type(r) for r in rows]))

    # Get a list of booleans corresponding to times we should highlight a weight in red (because it changed rapidly)
    weight_diff = W_ts[1:, instances_list] - W_ts[:-1, instances_list]  # shape [ts-1 inst d_in d_sae]
    weight_dist = np.pad(weight_diff.pow(2).sum(-2).sqrt().numpy(), ((1, 0), (0, 0), (0, 0)))  # shape [ts inst d_sae]
    df = pd.DataFrame(data=weight_dist.reshape(n_timesteps, -1))
    rolling_max_distances = (
        df.rolling(window=n_steps_to_highlight, min_periods=1).max().values.reshape(weight_dist.shape)
    )
    true_highlight_threshold = highlight_threshold if color_resampled_latents else 100
    is_highlighted = rolling_max_distances > true_highlight_threshold
    is_highlighted[: n_steps_to_highlight + 1, ...] = False  # don't highlight at start, shape [ts inst d_sae]

    # Populate row_data (this is data that's the same for all timesteps, and is specified for all plots within a row)
    batch_size_per_row = {}
    max_sizes = {}
    for row_type in present_row_types:
        all_data = t.concat(
            [data_log[i][r].flatten() for i in range(n_timesteps) for r in rows if get_row_type(r) == row_type]
        )
        nonzero_data = all_data[all_data.abs() > ZERO_THRESHOLD].abs()
        max_sizes[row_type] = float(nonzero_data.quantile(0.999).item()) * 1.4
        batch_size_per_row[row_type] = {"W": d_sae, "h": batch_size}[row_type]
    row_data = [
        {
            "subplot_titles": [html_row_name(row, inst) for inst in range(n_inst)],
            "marker_size": 5 if batch_size_per_row[get_row_type(row)] < 25 else 3.5,
            "line_width": 2 if batch_size_per_row[get_row_type(row)] < 25 else 1.5,
            "batch_size": batch_size_per_row[get_row_type(row)],
            "max_size": max_sizes[get_row_type(row)],
            "tickmarks": list(range(-int(max_sizes[get_row_type(row)]), int(max_sizes[get_row_type(row)]) + 1)),
        }
        for row in rows
    ]

    # Populate plot_data (by iterating over timesteps, then rows, then instances within each row)
    plot_data = []

    for ts in range(n_timesteps):
        for row_idx, row in enumerate(rows):
            row_type = "W" if "W_" in row else "h"

            # data.shape = [inst 2 batch/d_sae], colors.shape = [inst batch/d_sae]
            data = rearrange_fn(data_log_list[ts][row], row)[instances_list].numpy()  # shape [inst 2 batch]
            is_highlighted_all = t.from_numpy(
                is_highlighted[ts] if "W_" in row else np.full((n_inst, data.shape[-1]), False)
            ).tolist()

            # Get max reconstruction loss for this timestep, over all instances & models
            # all_L_recon = t.stack([data_log_list[ts][r] for r in h_row_types if "L_recon" in r]).sqrt()
            # L_recon_top = all_L_recon.flatten().topk(k=3).values[-1].item()
            L_recon_top = t.stack([data_log_list[ts][r] for r in h_row_types if "L_recon" in r]).max().item()
            L_recon = data_log_list[ts]["L_reconstruction"].tolist()

            # Add data for each instance in this row
            for inst_idx in range(n_inst):
                # We only add data that is non-zero, to save on file size
                nonzero_values = np.nonzero(np.absolute(data[inst_idx]).sum(0) > ZERO_THRESHOLD)[0].tolist()
                x = [round(x, 4) for x in data[inst_idx, 0, nonzero_values].tolist()]
                y = [round(y, 4) for y in data[inst_idx, 1, nonzero_values].tolist()]
                data_list = []
                for i, nz in enumerate(nonzero_values):
                    data_list.append({"x": x[i], "y": y[i], "index": nz})
                    if is_highlighted_all[inst_idx][nz]:
                        data_list[-1]["color"] = "red"
                    elif color_hidden_activations_by_loss and row_type == "h":
                        data_list[-1]["color"] = round(L_recon[nz][inst_idx] / L_recon_top, 2)
                plot_data.append({"timestep": ts, "row": row_idx, "inst": inst_idx, "data": data_list})

    # Get default (empty string) values for a bunch of javascript components
    play_button = ""
    change_title_script = ""

    if n_timesteps > 1:
        play_button = f"""<input type='range' id='slider' min='0' max='{n_timesteps-1}' value='0' step='1'>
<button class='button' onclick='togglePlayPause()'>Play/Pause</button>"""

        steps = [data_log[i]["steps"] for i in range(n_timesteps)]
        change_title_script = f"""
    var steps = {json.dumps(steps)}
    d3.select('#title').html(`{title}, step ${{steps[time_idx]}} / ${{steps[steps.length-1]}}`);
"""

    if (n_rows := len(rows)) > 1:
        plots_style = f"display: grid; grid-template-columns: repeat({n_inst}, 1fr); grid-gap: 10px;"

    # Create the actual HTML code
    html_template = f"""
<style>
    #title {{ font-family: sans-serif; }}
    #svg-containers {{
        font-family: sans-serif;
        {plots_style}
        .svg.plot {{ display: inline-block; width: {box_size}px; height: {box_size}px; }}
        .point {{ fill-opacity: 1.0; }}
        h1 {{ font-family: sans-serif; }}
    }}
    .tooltip {{
        position: absolute;
        background: #eee;
        padding: 6px;
        border: 1px solid black;
        border-radius: 5px;
        pointer-events: none;
        font-size: 12px;
        font-family: monospace;
    }}
</style>

<h1 id="title">{title if title is not None else ""}</h1>
<div id="svg-containers"></div>
{play_button}

<script src="https://d3js.org/d3.v7.min.js"></script>
<script>
    const data = {json.dumps(plot_data)};
    const row_data = {json.dumps(row_data)};

    const n_timesteps = {n_timesteps};
    const n_rows = {n_rows};
    const n_inst = {n_inst};
    const n_plots = n_inst * n_rows;

    const margin = {{top: 10, right: 10, bottom: 30, left: 30}};
    const width = {box_size} - margin.left - margin.right;
    const height = {box_size} - margin.top - margin.bottom;
    const origin = {{x: width / 2, y: height / 2}};

    const tooltip = d3.select("body")
        .append("div")
        .attr("class", "tooltip")
        .style("visibility", "hidden")

    const svgContainers = d3.select("#svg-containers")
        .selectAll("div")
        .data(d3.range(n_plots))
        .enter()
        .append("div")
        .attr("class", "plot-container")
        .each(function (d, i) {{
            const node = d3.select(this);
            node.append("div").html(row_data[Math.floor(i / n_inst)].subplot_titles[i % n_inst]);
            node.append("svg")
                .attr("class", "plot")
                .attr("width", width + margin.left + margin.right)
                .attr("height", height + margin.top + margin.bottom)
                .append("g")
                .attr("transform", `translate(${{margin.left}},${{margin.top}})`);
        }});


    const drawPlot = (plot_idx, time_idx) => {{
        const plot = d3.selectAll("svg.plot").filter((_, i) => i === plot_idx).select("g");
        plot.selectAll("*").remove();

        const row_idx = Math.floor(plot_idx / n_inst);
        const inst_idx = plot_idx % n_inst;
        var {{subplot_titles, marker_size, line_width, batch_size, max_size, tickmarks}} = row_data[row_idx];

        const x_scale = d3.scaleLinear().domain([-max_size, max_size]).range([0, width]);
        const y_scale = d3.scaleLinear().domain([-max_size, max_size]).range([height, 0]);

        const plot_data = data[time_idx * (n_inst * n_rows) + row_idx * n_inst + inst_idx];

        // if (batch_size < 20)
        plot.selectAll(".line")
            .data(plot_data.data)
            .enter().append("line")
            .attr("class", "line")
            .attr("x1", origin.x)
            .attr("y1", origin.y)
            .attr("x2", (d) => x_scale(d.x))
            .attr("y2", (d) => y_scale(d.y))
            .attr("stroke", (d, i) => getColor(d))
            .attr("stroke-width", line_width)

        plot.selectAll(".point")
            .data(plot_data.data)
            .enter().append("circle")
            .attr("class", "point")
            .attr("r", marker_size)
            .attr("cx", d => x_scale(d.x))
            .attr("cy", d => y_scale(d.y))
            .attr("fill", (d, i) => getColor(d))
            .each(function(d) {{ d.inst = inst_idx; d.row = row_idx; }})
            .on("mouseover", function(event, d) {{
                findConnectedPoints(d).each(function(pointData) {{
                    showTooltip(d3.select(this), event);
                }});
            }})
            .on("mouseout", function(event, d) {{
                findConnectedPoints(d).each(function(connectedPointData) {{
                    hideTooltip(d3.select(this));
                }});
            }});

        const axes = [
            {{ transform: `translate(0,${{height-1}})`, scale: d3.axisBottom(x_scale) }},
            {{ transform: `translate(1,0)`, scale: d3.axisLeft(y_scale) }},
            {{ transform: `translate(0,1)`, scale: d3.axisTop(x_scale) }},
            {{ transform: `translate(${{width-1}},0)`, scale: d3.axisRight(y_scale) }}
        ];
        axes.forEach((axis, index) => {{
            const g = plot.append("g")
                .attr("transform", axis.transform)
                .call(axis.scale.tickValues(index < 2 ? tickmarks : []).tickFormat(d3.format(".0f")).tickSize(0));
            g.selectAll("text")
                .attr("font-size", "13px")
                .attr(index === 0 ? "dy" : "dx", index === 0 ? "11px" : "-2px")
                .filter(index >= 2).remove();
        }});

        plot.selectAll(".domain")
            .style("stroke-width", "2px");  // Set stroke width

    }};

    const drawAllPlots = (time_idx) => {{
        d3.range(n_plots).forEach(plot_idx => drawPlot(plot_idx, time_idx));
        {change_title_script}
    }};

    drawAllPlots(0);

    let interval;
    let slider = d3.select('#slider');
    slider.on('input', function() {{ drawAllPlots(+this.value); }});

    function togglePlayPause() {{
        if (interval) {{
            clearInterval(interval);
            interval = null;
        }} else {{
            let currentIndex = +slider.property('value');
            interval = setInterval(
                function() {{
                    if (currentIndex < n_timesteps) {{
                        slider.property('value', currentIndex);
                        drawAllPlots(currentIndex);
                        currentIndex++;  // Increment the index for the next iteration
                    }} else {{
                        clearInterval(interval);  // Stop the interval when reaching the end
                    }}
                }},
                {1000 / fps}
            );
        }}
    }}

    function findConnectedPoints(d) {{
        // When we hover over a point, all other points on different rows with the same instance are highlighted (with the
        // added filter that hovering over weights can only highlight other weights, and vice-versa for hidden activations)
        
        return d3.selectAll(".point")
            .filter(p => p.index === d.index && p.inst === d.inst && row_data[d.row].batch_size === row_data[p.row].batch_size)
    }}

    function showTooltip(point, event) {{
        const pointData = point.data()[0];
        const pointCoords = point.node().getBoundingClientRect();

        point.attr("fill", "red").attr("r", row_data[pointData.row].marker_size * 1.5);

        const tooltip = d3.select("body").append("div")
            .attr("class", "tooltip")
            .style("visibility", "visible")
            .html(`x: ${{pointData.x.toFixed(4)}}, y: ${{pointData.y.toFixed(4)}}`)
            .style("top", (window.scrollY + pointCoords.top - 10) + "px")
            .style("left", (window.scrollX + pointCoords.left + 20) + "px");
    }}

    function hideTooltip(point) {{
        const pointData = point.data()[0];

        point.attr("fill", (d, i) => getColor(d)).attr("r", row_data[pointData.row].marker_size);
        d3.selectAll(".tooltip").remove();
    }}

    function getColor(d) {{
        if (!('color' in d)) return 'black';
        
        if (d.color === 'red') return 'red';
        
        if (typeof d.color === 'number') {{
            // Clamp the value between 0 and 1
            const value = Math.max(0, Math.min(1, d.color));
            // Convert to RGB where we're only increasing the red component
            return `rgb(${{Math.round(value * 255)}}, 0, 0)`;
        }}
        
        return 'black';  // Default case for any other values
    }}
</script>
    """

    if filename is not None:
        assert filename.endswith(".html"), "Filename must end in .html"
        with open(filename, "w") as file:
            file.write(html_template)
        print(f"Saved animation at {filename!r}")
    else:
        return html_template


def frac_active_line_plot(
    frac_active: Float[Tensor, "timesteps inst d_sae"],
    feature_probability: float = 0.025,
    log_freq: int = 100,
    title: str | None = None,
    width: int | None = 1000,
    height: int | None = None,
    y_max: float | None = 0.075,
    avg_window: int | None = None,
):
    if avg_window is not None:
        frac_active_df = pd.DataFrame(data=frac_active.flatten(1, -1)).rolling(window=avg_window).mean()
        frac_active = t.from_numpy(frac_active_df.values).reshape(frac_active.shape)

    n_steps, n_instances, d_sae = frac_active.shape

    y_max = y_max if (y_max is not None) else (feature_probability * 3)
    y_min = 0.0

    fig = go.Figure(
        layout=dict(
            template="simple_white",
            title=title,
            xaxis_title="Training Steps",
            yaxis_title="Fraction of datrapoints active",
            width=width,
            height=height,
            yaxis_range=[y_min, y_max],
        )
    )

    for inst in range(n_instances):
        for neuron in range(d_sae):
            fig.add_trace(
                go.Scatter(
                    x=list(range(0, log_freq * n_steps, log_freq)),
                    y=frac_active[:, inst, neuron].tolist(),
                    name=f"SAE latent #{neuron}",
                    mode="lines",
                    opacity=0.5,
                    legendgroup=f"Instance #{inst}",
                    legendgrouptitle_text=f"Instance #{inst}",
                )
            )
    fig.add_hline(
        y=feature_probability,
        opacity=1,
        line=dict(color="black", width=2),
        annotation_text="Feature prob",
        annotation_position="bottom left",
        annotation_font_size=14,
    )
    if avg_window is not None:
        fig.update_layout(xaxis_range=[log_freq * avg_window, log_freq * n_steps])

    fig.show()
    # return fig


def plot_feature_geometry(model: "Model", dim_fracs=None, filename: str | None = None):
    fig = px.line(
        x=1 / model.feature_probability[:, 0].cpu(),
        y=(model.cfg.d_hidden / (t.linalg.matrix_norm(model.W.detach(), "fro") ** 2)).cpu(),
        log_x=True,
        markers=True,
        template="ggplot2",
        height=600,
        width=1000,
        title="",
    )
    fig.update_layout(title="Number of Hidden Dimensions per Embedded Feature")
    fig.update_xaxes(title="1/(1-S), <-- dense | sparse -->")
    fig.update_yaxes(title="m/||W||_F^2")
    if dim_fracs is not None:
        dim_fracs = dim_fracs.detach().cpu().numpy()
        density = model.feature_probability[:, 0].cpu()

        for a, b in [(1, 2), (2, 3), (2, 5)]:
            val = a / b
            fig.add_hline(
                val,
                line_color="purple",
                opacity=0.2,
                line_width=1,
                annotation=dict(text=f"{a}/{b}"),
            )

        for a, b in [(3, 4), (3, 8), (3, 20)]:
            val = a / b
            fig.add_hline(
                val,
                line_color="purple",
                opacity=0.2,
                line_width=1,
                annotation=dict(text=f"{a}/{b}", x=0.05),
            )

        dx = 0
        for i in range(len(dim_fracs)):
            fracs_ = dim_fracs[i]
            N = fracs_.shape[0]
            xs = 1 / density
            if i != len(dim_fracs) - 1:
                dx = xs[i + 1] - xs[i]
            fig.add_trace(
                go.Scatter(
                    x=1 / density[i] * np.ones(N) + dx * np.random.uniform(-0.1, 0.1, N),
                    y=fracs_,
                    marker=dict(
                        color="black",
                        size=1,
                        opacity=0.5,
                    ),
                    mode="markers",
                )
            )
        fig.update_xaxes(showgrid=False)
        fig.update_yaxes(showgrid=False)
        fig.update_layout(showlegend=False, yaxis_title_text="Dimensionality, or m/||W||_F^2")
    fig.show()
    if filename is not None:
        fig.write_html(filename)


# ============= PART 3.2 UTILS =============


def get_tensor_size(obj):
    size = 0
    if t.is_tensor(obj):
        size += obj.element_size() * obj.nelement()
    return size


def get_tensors_size(obj):
    if isinstance(obj, t.nn.Module):
        return sum(get_tensor_size(p) for p in obj.parameters())
    if hasattr(obj, "state_dict"):
        return sum(get_tensor_size(t) for t in obj.state_dict().values())
    return get_tensor_size(obj)


def get_device(obj):
    if t.is_tensor(obj):
        return str(obj.device)
    if isinstance(obj, t.nn.Module):
        try:
            return str(next(iter(obj.parameters())).device)
        except StopIteration:
            return "N/A"
    return "N/A"


def print_memory_status():
    t.cuda.synchronize()
    allocated = t.cuda.memory_allocated(0)
    total = t.cuda.get_device_properties(0).total_memory
    free = total - allocated
    print(f"Allocated: {allocated / 1024**3:.2f} GB")
    print(f"Total:  {total / 1024**3:.2f} GB")
    print(f"Free:  {free / 1024**3:.2f} GB")


def profile_pytorch_memory(namespace: dict, n_top: int = 10, filter_device: str = None):
    print_memory_status()

    object_sizes = {}
    for name, obj in namespace.items():
        try:
            obj_type = (
                type(obj).__name__
                if isinstance(obj, t.nn.Module)
                else f"Tensor {tuple(obj.shape)}"
                if t.is_tensor(obj)
                else None
            )
            if obj_type is None:
                continue
            device = get_device(obj)
            if filter_device and device != filter_device:
                continue
            size = get_tensors_size(obj)
            object_sizes[name] = (obj_type, device, size / (1024**3))
        except (OpenAIError, ReferenceError):
            # OpenAIError: we can't inspect the type of certain objects without triggering API request
            # ReferenceError: this object might have been garbage collected, so we don't care about it
            continue

    # Convert bytes to GB, sort by size & print
    sorted_sizes = sorted(object_sizes.items(), key=lambda x: x[1][2], reverse=True)[:n_top]
    table_data = [(name, obj_type, device, size) for name, (obj_type, device, size) in sorted_sizes]
    print(
        tabulate(
            table_data, headers=["Name", "Object", "Device", "Size (GB)"], floatfmt=".2f", tablefmt="simple_outline"
        )
    )
