from dataclasses import dataclass
from typing import Callable, Literal

import einops
import torch as t
from jaxtyping import Float
from torch import Tensor, nn
from tqdm.auto import tqdm
import numpy as np

device = t.device(
    "mps"
    if t.backends.mps.is_available()
    else "cuda" if t.cuda.is_available() else "cpu"
)


def linear_lr(step, steps):
    return 1 - (step / steps)


def constant_lr(*_):
    return 1.0


def cosine_decay_lr(step, steps):
    return np.cos(0.5 * np.pi * step / (steps - 1))


@dataclass
class ToyModelConfig:
    # We optimize n_inst models in a single training loop to let us sweep over sparsity or importance
    # curves efficiently. You should treat the number of instances `n_inst` like a batch dimension,
    # but one which is built into our training setup. Ignore the latter 3 arguments for now, they'll
    # return in later exercises.
    n_inst: int
    n_features: int = 5
    d_hidden: int = 2
    n_correlated_pairs: int = 0
    n_anticorrelated_pairs: int = 0
    feat_mag_distn: Literal["unif", "normal"] = "unif"


class ToyModel(nn.Module):
    W: Float[Tensor, "inst d_hidden feats"]
    b_final: Float[Tensor, "inst feats"]

    # Our linear map (for a single instance) is x -> ReLU(W.T @ W @ x + b_final)

    def __init__(
        self,
        cfg: ToyModelConfig,
        feature_probability: float | Tensor = 0.01,
        importance: float | Tensor = 1.0,
        device=device,
    ):
        super(ToyModel, self).__init__()
        self.cfg = cfg

        if isinstance(feature_probability, float):
            feature_probability = t.tensor(feature_probability)
        self.feature_probability = feature_probability.to(device).broadcast_to(
            (cfg.n_inst, cfg.n_features)
        )
        if isinstance(importance, float):
            importance = t.tensor(importance)
        self.importance = importance.to(device).broadcast_to(
            (cfg.n_inst, cfg.n_features)
        )

        self.W = nn.Parameter(
            nn.init.xavier_normal_(t.empty((cfg.n_inst, cfg.d_hidden, cfg.n_features)))
        )
        self.b_final = nn.Parameter(t.zeros((cfg.n_inst, cfg.n_features)))
        self.to(device)

    def forward(
        self,
        features: Float[Tensor, "... inst feats"],
    ) -> Float[Tensor, "... inst feats"]:
        """
        Performs a single forward pass. For a single instance, this is given by:
            x -> ReLU(W.T @ W @ x + b_final)
        """
        h_input = einops.einsum(
            self.W, features, "... inst d_hid d_feat, ... inst d_feat -> ... inst d_hid"
        )
        h_out = einops.einsum(
            self.W, h_input, "... inst d_hid d_feat, ... inst d_hid -> ... inst d_feat"
        )
        return t.relu(h_out + self.b_final)

    # My first implementation:

    # def generate_batch(self, batch_size: int) -> Float[Tensor, "batch inst feats"]:
    #     """
    #     Generates a batch of data of shape (batch_size, n_instances, n_features).
    #     This should return a tensor of shape (n_batch, instances, features), where:

    #         - The instances and features values are taken from the model config,
    #         - Each feature is present with probability self.feature_probability,
    #         - For each present feature, its magnitude is sampled from a uniform distribution between 0 and 1.
    #     """

    #     instances = self.cfg.n_inst
    #     features = self.cfg.n_features
    #     full_shape = batch_size, instances, features
    #     features_extraction = t.rand(full_shape, device=self.W.device)
    #     features_mag = t.rand(full_shape, device=self.W.device)
    #     batch = (features_extraction <= self.feature_probability) * features_mag

    #     return batch

    # Solution with correlations:
    def generate_batch(self, batch_size) -> Float[Tensor, "batch inst feats"]:
        """
        Generates a batch of data, with optional correlated & anticorrelated features.
        """
        n_corr_pairs = self.cfg.n_correlated_pairs
        n_anti_pairs = self.cfg.n_anticorrelated_pairs
        n_uncorr = self.cfg.n_features - 2 * n_corr_pairs - 2 * n_anti_pairs

        data = []
        if n_corr_pairs > 0:
            data.append(self.generate_correlated_features(batch_size, n_corr_pairs))
        if n_anti_pairs > 0:
            data.append(self.generate_anticorrelated_features(batch_size, n_anti_pairs))
        if n_uncorr > 0:
            data.append(self.generate_uncorrelated_features(batch_size, n_uncorr))
        batch = t.cat(data, dim=-1)
        return batch

    def calculate_loss(
        self,
        out: Float[Tensor, "batch inst feats"],
        batch: Float[Tensor, "batch inst feats"],
    ) -> Float[Tensor, ""]:
        """
        Calculates the loss for a given batch (as a scalar tensor), using this loss described in the
        Toy Models of Superposition paper:

            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss

        Note, `self.importance` is guaranteed to broadcast with the shape of `out` and `batch`.
        """

        diff = ((out - batch) ** 2) * self.importance

        rme = einops.reduce(diff, "batch inst feats -> inst", "mean")
        return t.sum(rme)

    def optimize(
        self,
        batch_size: int = 1024,
        steps: int = 5_000,
        log_freq: int = 50,
        lr: float = 1e-3,
        lr_scale: Callable[[int, int], float] = constant_lr,
    ):
        """
        Optimizes the model using the given hyperparameters.
        """
        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)

        progress_bar = tqdm(range(steps))

        for step in progress_bar:
            # Update learning rate
            step_lr = lr * lr_scale(step, steps)
            for group in optimizer.param_groups:
                group["lr"] = step_lr

            # Optimize
            optimizer.zero_grad()
            batch = self.generate_batch(batch_size)
            out = self(batch)
            loss = self.calculate_loss(out, batch)
            loss.backward()
            optimizer.step()

            # Display progress bar
            if step % log_freq == 0 or (step + 1 == steps):
                progress_bar.set_postfix(loss=loss.item() / self.cfg.n_inst, lr=step_lr)

    def generate_correlated_features(
        self, batch_size: int, n_correlated_pairs: int
    ) -> Float[Tensor, "batch inst 2*n_correlated_pairs"]:
        """
        Generates a batch of correlated features. For each pair `batch[i, j, [2k, 2k+1]]`, one of
        them is non-zero if and only if the other is non-zero.
        """
        assert t.all((self.feature_probability == self.feature_probability[:, [0]]))
        p = self.feature_probability[:, [0]]  # shape (n_inst, 1)

        feat_mag = t.rand(
            (batch_size, self.cfg.n_inst, 2 * n_correlated_pairs), device=self.W.device
        )
        feat_set_seeds = t.rand(
            (batch_size, self.cfg.n_inst, n_correlated_pairs), device=self.W.device
        )
        feat_set_is_present = feat_set_seeds <= p
        feat_is_present = einops.repeat(
            feat_set_is_present,
            "batch instances features -> batch instances (features pair)",
            pair=2,
        )
        return t.where(feat_is_present, feat_mag, 0.0)

    def generate_anticorrelated_features(
        self, batch_size: int, n_anticorrelated_pairs: int
    ) -> Float[Tensor, "batch inst 2*n_anticorrelated_pairs"]:
        """
        Generates a batch of anti-correlated features. For each pair `batch[i, j, [2k, 2k+1]]`, each
        of them can only be non-zero if the other one is zero.
        """
        print("Gen corr")
        assert t.all((self.feature_probability == self.feature_probability[:, [0]]))
        p = self.feature_probability[:, [0]]  # shape (n_inst, 1)

        assert p.max().item() <= 0.5, "For anticorrelated features, must have 2p < 1"

        feat_mag = t.rand(
            (batch_size, self.cfg.n_inst, 2 * n_anticorrelated_pairs),
            device=self.W.device,
        )
        even_feat_seeds, odd_feat_seeds = t.rand(
            (2, batch_size, self.cfg.n_inst, n_anticorrelated_pairs),
            device=self.W.device,
        )
        even_feat_is_present = even_feat_seeds <= p
        odd_feat_is_present = (even_feat_seeds > p) & (odd_feat_seeds <= p / (1 - p))
        feat_is_present = einops.rearrange(
            t.stack([even_feat_is_present, odd_feat_is_present], dim=0),
            "pair batch instances features -> batch instances (features pair)",
        )
        return t.where(feat_is_present, feat_mag, 0.0)

    def generate_uncorrelated_features(
        self, batch_size: int, n_uncorrelated: int
    ) -> Tensor:
        """
        Generates a batch of uncorrelated features.
        """
        if n_uncorrelated == self.cfg.n_features:
            p = self.feature_probability
        else:
            assert t.all((self.feature_probability == self.feature_probability[:, [0]]))
            p = self.feature_probability[:, [0]]  # shape (n_inst, 1)

        if n_uncorrelated == self.cfg.n_features:
            p = self.feature_probability
        else:
            assert t.all((self.feature_probability == self.feature_probability[:, [0]]))
            p = self.feature_probability[:, [0]]  # shape (n_inst, 1)

        feat_mag = t.rand(
            (batch_size, self.cfg.n_inst, n_uncorrelated), device=self.W.device
        )
        feat_seeds = t.rand(
            (batch_size, self.cfg.n_inst, n_uncorrelated), device=self.W.device
        )
        return t.where(feat_seeds <= p, feat_mag, 0.0)
